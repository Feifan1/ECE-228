{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pathlib \n",
    "import numpy as np\n",
    "import scipy.io as sio \n",
    "import math\n",
    "import keras\n",
    "# tf.enable_eager_execution()\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = './dataset/cars_train'\n",
    "data_root = pathlib.Path(data_root)\n",
    "\n",
    "all_image_paths = sorted([str(path) for path in data_root.iterdir()])\n",
    "\n",
    "len(all_image_paths)\n",
    "\n",
    "traindata = pd.read_csv('./dataset/traindata.csv', index_col=0)\n",
    "\n",
    "# train_paths = all_image_paths\n",
    "train_labels = traindata['class'].tolist()\n",
    "\n",
    "train_bbox = traindata[['bbox1', 'bbox2', 'bbox3', 'bbox4']].values.tolist()\n",
    "train_bbox = np.array(train_bbox, dtype=np.str)\n",
    "\n",
    "all_image_paths = np.array(all_image_paths).reshape((-1, 1))\n",
    "\n",
    "all_image_paths = np.concatenate((all_image_paths, train_bbox), axis=1)\n",
    "\n",
    "def preprocess_image(image, bbox):\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.crop_to_bounding_box(image, bbox[1], bbox[0], bbox[3]-bbox[1],\n",
    "                                          bbox[2] - bbox[0])\n",
    "    image = tf.image.resize_images(image, [224, 224])\n",
    "#     image = (image - 127.5)/127.5  # normalize to [-1,1] range\n",
    "    image = tf.image.per_image_standardization(image)\n",
    "#     print(image.math.maximum)\n",
    "    return image\n",
    "\n",
    "def load_and_preprocess_image(path):\n",
    "    image = tf.read_file(path[0])\n",
    "    return preprocess_image(image, tf.string_to_number(path[1:], out_type=tf.int32))\n",
    "\n",
    "class_dir = \"./cars_meta.mat\"\n",
    "def getclass(class_dir):\n",
    "    annos = sio.loadmat(class_dir)\n",
    "    _, num_class = annos['class_names'].shape\n",
    "    class_label = {i:annos['class_names'][0,i][0] for i in range(num_class)}\n",
    "    return class_label\n",
    "class_label = getclass(class_dir)\n",
    "\n",
    "def caption_image(image_path):\n",
    "    image_rel = pathlib.Path(image_path).relative_to(data_root)\n",
    "    return str(image_rel)\n",
    "caption_image(all_image_paths[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ds = tf.data.Dataset.from_tensor_slices(all_image_paths)\n",
    "image_ds = path_ds.map(load_and_preprocess_image)\n",
    "\n",
    "label_ds = tf.data.Dataset.from_tensor_slices(tf.cast(np.array(train_labels) - 1, tf.int64))\n",
    "\n",
    "image_label_ds = tf.data.Dataset.zip((image_ds, label_ds))\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "image_count = len(all_image_paths)\n",
    "# Setting a shuffle buffer size as large as the dataset ensures that the data is\n",
    "# completely shuffled.\n",
    "ds = image_label_ds.shuffle(buffer_size=image_count)\n",
    "ds = ds.repeat()\n",
    "ds = ds.batch(BATCH_SIZE)\n",
    "# `prefetch` lets the dataset fetch batches, in the background while the model is training.\n",
    "# ds = ds.prefetch(buffer_size=64)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_np = np.array([train_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_image_path_all = np.concatenate((all_image_paths, train_labels_np.T), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_image_path_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size=32, dim=(32,224,224), n_channels=3,\n",
    "                 n_classes=196, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.dim[0] = batch_size\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            with Image.open(ID[0]) as img:\n",
    "                img = img.crop(box=(ID[1],ID[2],ID[3],ID[4])).resize((224,224))\n",
    "                img = np.array(img)\n",
    "                X[i,] = img\n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.ID[-1]\n",
    "\n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata = pd.read_csv('./dataset/testdata.csv', index_col=0) #\n",
    "\n",
    "test_data_root = './dataset/cars_test' # /dataset\n",
    "test_data_root = pathlib.Path(test_data_root)\n",
    "\n",
    "test_paths = sorted([str(path) for path in test_data_root.iterdir()])\n",
    "\n",
    "len(test_paths)\n",
    "\n",
    "test_labels = testdata['class'].tolist()\n",
    "\n",
    "test_bbox = testdata[['bbox1', 'bbox2', 'bbox3', 'bbox4']].values.tolist()\n",
    "test_bbox = np.array(test_bbox, dtype=np.str)\n",
    "\n",
    "test_paths = np.array(test_paths).reshape((-1, 1))\n",
    "\n",
    "test_paths = np.concatenate((test_paths, test_bbox), axis=1)\n",
    "\n",
    "test_path_ds = tf.data.Dataset.from_tensor_slices(test_paths)\n",
    "test_image_ds = test_path_ds.map(load_and_preprocess_image)\n",
    "\n",
    "test_label_ds = tf.data.Dataset.from_tensor_slices(tf.cast(np.array(test_labels) - 1, tf.int64))\n",
    "\n",
    "test_image_label_ds = tf.data.Dataset.zip((test_image_ds, test_label_ds))\n",
    "test_image_label_ds = test_image_label_ds.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.applications.inception_v3 import InceptionV3\n",
    "# from keras.preprocessing import image\n",
    "# from keras.models import Model\n",
    "# from keras.layers import Dense, GlobalAveragePooling2D\n",
    "# from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 32\n",
    "# # tf.enable_eager_execution\n",
    "# epochs = 5\n",
    "# num_classes = 196"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollectBatchStats(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.batch_losses = []\n",
    "        self.batch_acc = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.batch_losses.append(logs['loss'])\n",
    "        self.batch_acc.append(logs['acc'])\n",
    "\n",
    "checkPoint = tf.keras.callbacks.ModelCheckpoint(filepath='./ece228_project/weights_incept.h5', save_weights_only=True, period=4)\n",
    "batch_stats = CollectBatchStats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "# # add a global spatial average pooling layer\n",
    "# x = base_model.output\n",
    "# x = GlobalAveragePooling2D()(x)\n",
    "# # let's add a fully-connected layer\n",
    "# x = Dense(1024, activation='relu')(x)\n",
    "# # and a logistic layer -- let's say we have 200 classes\n",
    "# predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# # this is the model we will train\n",
    "# model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# # first: train only the top layers (which were randomly initialized)\n",
    "# # i.e. freeze all convolutional InceptionV3 layers\n",
    "# for layer in base_model.layers:\n",
    "#     layer.trainable = False\n",
    "\n",
    "# # compile the model (should be done *after* setting layers to non-trainable)\n",
    "# model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# # train the model on the new data for a few epochs\n",
    "# model.fit(ds,  epochs=epochs,\\\n",
    "#                   validation_data=None, verbose=1, steps_per_epoch=math.ceil(8144/32), callbacks=None) # [batch_stats, checkPoint]\n",
    "\n",
    "# # at this point, the top layers are well trained and we can start fine-tuning\n",
    "# # convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# # and train the remaining top layers.\n",
    "\n",
    "# # let's visualize layer names and layer indices to see how many layers\n",
    "# # we should freeze:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, layer in enumerate(base_model.layers):\n",
    "#    print(i, layer.name)\n",
    "\n",
    "# # we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# # the first 249 layers and unfreeze the rest:\n",
    "# for layer in model.layers[:249]:\n",
    "#    layer.trainable = False\n",
    "# for layer in model.layers[249:]:\n",
    "#    layer.trainable = True\n",
    "\n",
    "# # we need to recompile the model for these modifications to take effect\n",
    "# # we use SGD with a low learning rate\n",
    "# from keras.optimizers import SGD\n",
    "# model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')\n",
    "\n",
    "# # we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# # alongside the top Dense layers\n",
    "# model.fit_generator(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BASELINE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './ece228_project'\n",
    "batch_size = 32\n",
    "# tf.enable_eager_execution\n",
    "epochs = 200\n",
    "num_classes = 196\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollectBatchStats(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.batch_losses = []\n",
    "        self.batch_acc = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.batch_losses.append(logs['loss'])\n",
    "        self.batch_acc.append(logs['acc'])\n",
    "\n",
    "checkPoint = tf.keras.callbacks.ModelCheckpoint(filepath='./ece228_project/weights.h5', save_weights_only=True, period=99)\n",
    "batch_stats = CollectBatchStats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBnReluBlock(tf.keras.Model):\n",
    "    def __init__(self, filters, kernel, strides):\n",
    "        super(ConvBnReluBlock, self).__init__()\n",
    "        self.cnn = tf.keras.layers.Conv2D(filters, (kernel, kernel), strides=(strides, strides), kernel_initializer='he_normal',activation=tf.keras.activations.relu)\n",
    "        self.bn = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        x = self.cnn(inputs)\n",
    "        x = self.bn(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "\n",
    "class CNN(tf.keras.Model):\n",
    "    def __init__(self, num_classes=196):\n",
    "        super(CNN, self).__init__()\n",
    "        self.block1 = ConvBnReluBlock(32, kernel=5, strides=1)\n",
    "        self.block2 = ConvBnReluBlock(64, kernel=5, strides=1)\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=2)\n",
    "        self.pool2 = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        self.fullconnect = tf.keras.layers.Dense(1024)\n",
    "        self.classifier = tf.keras.layers.Dense(num_classes)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        x = self.block1(inputs)\n",
    "        x = self.pool1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.fullconnect(x)\n",
    "        output = self.classifier(x)\n",
    "\n",
    "        # softmax op does not exist on the gpu, so always use cpu\n",
    "        with tf.device('/cpu:0'):\n",
    "            output = tf.nn.softmax(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "device = '/gpu:0' \n",
    "# if tf.num_gpus() == 0 else '/gpu:0'\n",
    "\n",
    "# with tf.device(device):\n",
    "    # build model and optimizer\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "# evaluate on test set\n",
    "# scores = model.evaluate(test_image_label_ds, batch_size=batch_size, verbose=1, steps=math.ceil(8144/32))\n",
    "# print(\"Final test loss and accuracy :\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train):\n",
    "    dummy_x = tf.zeros((1 ,224, 224, 3))\n",
    "    if train:\n",
    "        model = CNN(num_classes)\n",
    "\n",
    "\n",
    "        # TF Keras tries to use entire dataset to determine shape without this step when using .fit()\n",
    "        # Fix = Use exactly one sample from the provided input dataset to determine input/output shape/s for the model\n",
    "        \n",
    "        model._set_inputs(dummy_x)\n",
    "\n",
    "        model.compile(optimizer=tf.train.AdamOptimizer(0.001), loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        # train\n",
    "        model.fit(ds,  epochs=epochs,\\\n",
    "                  validation_data=None, verbose=1, steps_per_epoch=math.ceil(8144/32), callbacks=[batch_stats, checkPoint])\n",
    "    else:\n",
    "        test_model = CNN(num_classes)\n",
    "        test_model._set_inputs(dummy_x)\n",
    "        test_model.compile(optimizer=tf.train.AdamOptimizer(0.001), loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "        test_model.load_weights(filepath='./ece228_project/weights.h5')\n",
    "        scores = test_model.evaluate(test_image_label_ds, batch_size=batch_size, verbose=1, steps=tf.ceil(len(test_paths)/BATCH_SIZE).numpy().astype(np.int))\n",
    "        print(\"Final test loss and accuracy :\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_x = tf.zeros((1 ,224, 224, 3))\n",
    "\n",
    "test_model = CNN(num_classes)\n",
    "test_model._set_inputs(dummy_x)\n",
    "test_model.compile(optimizer=tf.train.AdamOptimizer(0.001), loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "test_model.load_weights(filepath='./ece228_project/weights.h5')\n",
    "scores = test_model.evaluate(test_image_label_ds, batch_size=batch_size, verbose=1, steps=tf.ceil(len(test_paths)/BATCH_SIZE).numpy().astype(np.int))\n",
    "print(\"Final test loss and accuracy :\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylim([0,10])\n",
    "plt.plot(batch_stats.batch_losses)\n",
    "\n",
    "plt.figure()\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylim([0,1])\n",
    "plt.plot(batch_stats.batch_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
